library(truncnorm)
a=1
b=0.1

pf=function(y, N) {
  # Initialize particles
  particles=data.frame(
    phi=exp(rtruncnorm(N,a=-1,b=1,mean=0.1,sd=2)),
    tau=rgamma(N,a,b),
    w=runif(N,0,1000),
    x=7.75
  )
  
  updated.particles=list()
  
  # Iterating over each time point
  for (t in 1:length(y)) {
    # Update particles
    epsilon=rnorm(N,0.01,sqrt(1/particles$tau))
    particles$x=particles$phi*particles$x+epsilon
    
    # Calculate likelihood weights
    weights=dnorm(y[t], mean=particles$x, sd=sqrt(1/particles$w))
    weights=weights/sum(weights)
    
    # Resampling
    index=sample(1:N, N, replace = TRUE, prob = weights)
    particles=particles[index, ]
    
    # Store updated particles
    updated.particles[[t]]=particles
  }
  
  return(updated.particles)
}



set.seed(5090)
N=5000 # Number of particles  
#y=arima.sim(n=100, list(ar=0.8))  # Simulated observation data
y=read.csv("C://Users//Thf12//OneDrive - University of Canterbury//STAT395//USD_HKD2.csv")
y$date <- as.Date(y$date, format="%d/%m/%Y")
results=pf(y[,2], N)



# 提取最后时间点的粒子
last_particles=results[[length(y)]]

# 计算每个参数的后验统计量
posterior_summary=function(particles, param) {
  mean_val=mean(particles[[param]])
  sd_val=sd(particles[[param]])
  ci=quantile(particles[[param]], probs = c(0.1, 0.9))
  list(mean = mean_val, sd = sd_val, ci = ci)
}

summary_phi=posterior_summary(last_particles, "phi")
summary_tau =posterior_summary(last_particles, "tau")
summary_w=posterior_summary(last_particles, "w")

# 打印统计量
print(summary_phi)
print(summary_tau)
print(summary_w)











